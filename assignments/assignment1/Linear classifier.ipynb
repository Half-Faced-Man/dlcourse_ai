{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "(0,)\n",
      "analytic_grad_at_ix =  6.0\n",
      "numeric_grad_at_ix =  6.000000000039306\n",
      "Gradient check passed!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "(0,)\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "(1,)\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "Gradient check passed!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "(0, 0)\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "(0, 1)\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "(1, 0)\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "(1, 1)\n",
      "analytic_grad_at_ix =  1.0\n",
      "numeric_grad_at_ix =  0.9999999999621422\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient_2(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient_2(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient_2(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs = linear_classifer.softmax( np.array([[-5, 0, 5] , [-9 , 0 , 6]]) )\n",
    "# print(probs)\n",
    "# linear_classifer.cross_entropy_loss(probs, np.array([[1, 0, 0] , [0 , 0 , 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient( lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[ 1 , 2 , -1 ,  1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ 1 , 2 , -1 ,  1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = np.array([[ 1 , 2 , -1 ,  1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.array([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2, -1,  1]])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(qq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0]])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.reshape(pp.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(pp.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[ np.arange(pp.shape[0])  ,  pp.reshape(pp.shape[0] )] =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0]])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = np.array([[ 1 , 2 , -1 ,  1] , [2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = np.array([[2] , [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2, -1,  1],\n",
       "       [ 2,  3,  4,  5]])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(qq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.reshape(pp.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(pp.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[ np.arange(pp.shape[0])  ,  pp.reshape(pp.shape[0] )] =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "# print(W)\n",
    "# print(X)\n",
    "# print(target_index)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LinearSoftmaxClassifier_test():\n",
    "#     def __init__(self):\n",
    "#         self.W = None\n",
    "\n",
    "#     def fit(self, X, y, batch_size=100, learning_rate=1e-7, reg=1e-5,\n",
    "#             epochs=1):\n",
    "#         '''\n",
    "#         Trains linear classifier\n",
    "        \n",
    "#         Arguments:\n",
    "#           X, np array (num_samples, num_features) - training data\n",
    "#           y, np array of int (num_samples) - labels\n",
    "#           batch_size, int - batch size to use\n",
    "#           learning_rate, float - learning rate for gradient descent\n",
    "#           reg, float - L2 regularization strength\n",
    "#           epochs, int - number of epochs\n",
    "#         '''\n",
    "\n",
    "#         num_train = X.shape[0]\n",
    "#         num_features = X.shape[1]\n",
    "#         num_classes = np.max(y)+1\n",
    "#         if self.W is None:\n",
    "#             self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "\n",
    "#         loss_history = []\n",
    "#         for epoch in range(epochs):\n",
    "#             shuffled_indices = np.arange(num_train)\n",
    "#             np.random.shuffle(shuffled_indices)\n",
    "#             sections = np.arange(batch_size, num_train, batch_size)\n",
    "#             batches_indices = np.array_split(shuffled_indices, sections)\n",
    "\n",
    "#             # TODO implement generating batches from indices\n",
    "#             # Compute loss and gradients\n",
    "#             # Apply gradient to weights using learning rate\n",
    "#             # Don't forget to add both cross-entropy loss\n",
    "#             # and regularization!\n",
    "            \n",
    "# #             print(shuffled_indices.shape)\n",
    "# #             print(sections.shape)\n",
    "# #             print(sections)\n",
    "# #             print(len(batches_indices))\n",
    "# #             print(batches_indices[0])\n",
    "            \n",
    "#             for batch in batches_indices:\n",
    "#                 X_train = X[batch]\n",
    "#                 y_train = y[batch]\n",
    "                \n",
    "#                 loss, dW = linear_classifer.linear_softmax(X_train, self.W, y_train)\n",
    "#                 reg_loss , reg_dW = linear_classifer.l2_regularization(self.W, reg)\n",
    "                \n",
    "#                 loss = loss + reg_loss\n",
    "#                 dW = dW + reg_dW\n",
    "                \n",
    "#                 self.W = self.W - learning_rate * dW\n",
    "                \n",
    "                \n",
    "#             loss_history.append(loss)\n",
    "\n",
    "\n",
    "#             # end\n",
    "#             #print(\"Epoch %i, loss: %f\" % (epoch, loss))\n",
    "\n",
    "#         return loss_history\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         '''\n",
    "#         Produces classifier predictions on the set\n",
    "       \n",
    "#         Arguments:\n",
    "#           X, np array (test_samples, num_features)\n",
    "\n",
    "#         Returns:\n",
    "#           y_pred, np.array of int (test_samples)\n",
    "#         '''\n",
    "#         y_pred = np.zeros(X.shape[0], dtype=np.int)\n",
    "#         # print(y_pred.shape)\n",
    "\n",
    "#         # TODO Implement class prediction\n",
    "#         # Your final implementation shouldn't have any loops\n",
    "#         predictions = np.dot(X , self.W)\n",
    "#         predictions = linear_classifer.softmax(predictions)\n",
    "#         y_pred = np.argsort(predictions , axis = 1)# [: , 0:1]# [: , 0:self.k]\n",
    "#         tst = np.argmax(predictions , axis = 1)\n",
    "# #         print(predictions[:10])\n",
    "# #         print(y_pred[:10])\n",
    "# #         print(tst[:10])\n",
    "        \n",
    "        \n",
    "#         return tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11d0469d0>]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.165\n",
      "Accuracy after training for 100 epochs:  0.158\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.0001 , reg_strength = 0.0001 , accuracy = 0.251\n",
      "learning_rate = 0.0001 , reg_strength = 0.001 , accuracy = 0.255\n",
      "learning_rate = 0.0001 , reg_strength = 0.01 , accuracy = 0.248\n",
      "learning_rate = 0.0001 , reg_strength = 0.1 , accuracy = 0.245\n",
      "learning_rate = 0.0001 , reg_strength = 1.0 , accuracy = 0.247\n",
      "learning_rate = 0.001 , reg_strength = 0.0001 , accuracy = 0.194\n",
      "learning_rate = 0.001 , reg_strength = 0.001 , accuracy = 0.21\n",
      "learning_rate = 0.001 , reg_strength = 0.01 , accuracy = 0.189\n",
      "learning_rate = 0.001 , reg_strength = 0.1 , accuracy = 0.2\n",
      "learning_rate = 0.001 , reg_strength = 1.0 , accuracy = 0.152\n",
      "learning_rate = 0.01 , reg_strength = 0.0001 , accuracy = 0.171\n",
      "learning_rate = 0.01 , reg_strength = 0.001 , accuracy = 0.131\n",
      "learning_rate = 0.01 , reg_strength = 0.01 , accuracy = 0.153\n",
      "learning_rate = 0.01 , reg_strength = 0.1 , accuracy = 0.188\n",
      "learning_rate = 0.01 , reg_strength = 1.0 , accuracy = 0.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a17409438/Documents/dlcourse_ai/assignments/assignment1/linear_classifer.py:80: RuntimeWarning: divide by zero encountered in log\n",
      "  return (-1) * np.sum( np.log(loss) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.1 , reg_strength = 0.0001 , accuracy = 0.111\n",
      "learning_rate = 0.1 , reg_strength = 0.001 , accuracy = 0.141\n",
      "learning_rate = 0.1 , reg_strength = 0.01 , accuracy = 0.108\n",
      "learning_rate = 0.1 , reg_strength = 0.1 , accuracy = 0.155\n",
      "learning_rate = 0.1 , reg_strength = 1.0 , accuracy = 0.141\n",
      "learning_rate = 1.0 , reg_strength = 0.0001 , accuracy = 0.119\n",
      "learning_rate = 1.0 , reg_strength = 0.001 , accuracy = 0.177\n",
      "learning_rate = 1.0 , reg_strength = 0.01 , accuracy = 0.117\n",
      "learning_rate = 1.0 , reg_strength = 0.1 , accuracy = 0.137\n",
      "learning_rate = 1.0 , reg_strength = 1.0 , accuracy = 0.072\n",
      "best validation accuracy achieved: 0.255000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "# learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "# reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "learning_rates = [0.0001 , 0.001, 0.01, 0.1, 1.0]\n",
    "reg_strengths = [0.0001 , 0.001, 0.01, 0.1,  1.0]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = LinearSoftmaxClassifier_test()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print( \"learning_rate = {} , reg_strength = {} , accuracy = {}\".format( lr , rs , accuracy) )\n",
    "        accuracy_list.append((lr , rs , accuracy )) \n",
    "        \n",
    "        if best_classifier is None:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "        \n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = 0.001 , reg_strength = 0.1 , accuracy = 0.207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.217000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
